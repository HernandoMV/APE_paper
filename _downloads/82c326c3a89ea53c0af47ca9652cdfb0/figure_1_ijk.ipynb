{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Figure 1 IJK\nrecreate figure 1IJK from the paper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook performs a statistical analysis on the learning rates between two groups of mice.\nPerforms model fitting of individual mice, comparison of parameter, and statistical analysis\nbetween the groups to conclude differences and points of divergence in learning\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n# run this on Colab\n# !rm -rf APE_paper/\n# !git clone https://github.com/HernandoMV/APE_paper.git\n# %pip install mouse-behavior-analysis-tools\n# %cd APE_paper/docs/figures_notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n# %load_ext autoreload\n# %autoreload 2\nimport matplotlib.pylab as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom scipy import stats\nfrom IPython.display import clear_output\nfrom itertools import chain\nfrom os.path import exists\nimport urllib.request\n\nfrom mouse_behavior_analysis_tools.utils import custom_functions as cuf\nfrom mouse_behavior_analysis_tools.plot import make_figures\nfrom mouse_behavior_analysis_tools.model import behavior_model as bm\n\nclear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This dataset has been pre-processed, but conditions have not been selected\nThis preprocessing includes removal of disengaged trials and\nremoval of the first 5 trials of each session\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 2. Download data\n# ~~~~~~~~~~~~~~~~~\ndataset_name = 'Chronic-lesions_dataframe.csv'\nurl = \"https://zenodo.org/record/7261639/files/\" + dataset_name\ndataset_path = '../data/' + dataset_name\n# download if data is not there\nif not exists(dataset_path):\n    print('Downloading data...')\n    urllib.request.urlretrieve(url, dataset_path)\nelse:\n    print('Data already in directory')\n# load\ndf_to_plot = pd.read_csv(dataset_path, index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "parameters for the plotting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hue_order = ['Control', 'Lesion']\ncolor_palette = [(0.24715576, 0.49918708, 0.57655991),\n                 (160/255, 11/255 , 11/255)]\nsns.set_palette(color_palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "maximum number of trials performed per mouse in the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_to_plot.groupby(['AnimalID', 'ExperimentalGroup', 'Protocol']).max()['CumulativeTrialNumberByProtocol']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Plot performance\nbin trials every 200\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_to_plot[\"TrialIndexBinned200\"] = (df_to_plot.CumulativeTrialNumberByProtocol // 200) * 200 + 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sanity check on the data to see that it looks good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = make_figures.make_figure_performance_trials_animals_bin(df_to_plot)\nclear_output()\nplt.show(fig)\n# uncomment here to save the plot\n# data_directory = ''\n# plt.savefig(data_directory + 'Performance_by_session_individual_animals.pdf',\n#             transparent=True, bbox_inches='tight')\n\n\"\"\"Colored dots show the performance of the past 100 trials using an average running window.\nEach color represents a distinct session. The black line shows the performance value of\nthe past 200 trials using trial bining.\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Model fitting\n ~~~~~~~~~~~~~~~~~~~~~\n### Fit a sigmoid to every mouse to calculate and compare stages and learning rates\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\nThe sigmoid function is (perf_end - 0.5) / (1 + np.exp(-slope * (x - bias))) + 0.5\n\nData is scaled before the fitting and parameters are rescaled afterwards\n\nThe maximum performace possible is defined as the maximum of the median of the trials binned every 200\n\"\"\"\n\n#calculate the maximum performance for every mouse based on the trials binned every 200\ndf_bin200tr = df_to_plot.groupby(['AnimalID','ExperimentalGroup','TrialIndexBinned200','Protocol']).median().reset_index()\nmouse_max_perf = df_bin200tr.groupby('AnimalID').max().reset_index()[['AnimalID', 'CurrentPastPerformance100']]\n\n# fit the model\nfit_df = bm.get_df_of_behavior_model_by_animal(df_to_plot, mouse_max_perf)\n\n# find the steepest point of each slope (max of derivative)\nder_max_dir = bm.get_steepest_point_of_slope(fit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "plot the curves again pointing to the maximum\nsanity check to see that these scaled values recreate the curves\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = make_figures.make_figure_performance_trials_animals_model(df_to_plot, fit_df, der_max_dir)\nclear_output()\nplt.show(fig)\n# plt.savefig(data_directory + 'Sigmoid_fitting.pdf', transparent=True, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"As a learning speed measure I use the point of the maximum learning rate, which is the maximum of the derivative of the fitting, which is also the middle point of the curve (in between 50% and maximum performance)\nThe bias indicates where this point lies in the x axis: at which trial number this point is reached\n\"\"\"\n\n# add the maximum value of the derivative to the fit_df dataframe\nfor key, value in der_max_dir.items():\n    fit_df.loc[fit_df.index[fit_df['AnimalID'] == key].tolist()[0], 'max_of_der'] = value[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Statistics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"### Differences of parameters between groups\"\"\"\n\n# calculate significance\nparameters_to_show = ['maximum_performance', 'max_of_der']\n# define three levels of significance for the plotting\nsig_levels = [0.05, 0.01, 0.001]\npvals = []\nprint('Kluskal-Wallis tests on the parameters')\nfor var in parameters_to_show:\n    kt = stats.kruskal(fit_df[fit_df.ExperimentalGroup==hue_order[0]][var].dropna(),\n                       fit_df[fit_df.ExperimentalGroup==hue_order[1]][var].dropna())\n    print( var + ':\\t\\tpvalue: ' + str(kt.pvalue) )\n    pvals.append(kt.pvalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plot figure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"##### The following cell creates **Figures 1 J, K** from the paper\"\"\"\n\n# compare the parameters between groups\ntitles = ['maximum\\nperformance', 'maximum\\nlearning rate']\nylabs = ['task performance (%)', '\\u0394performance \\u00D7 trials$\\mathregular{^{-1}}$']\nyvals = [70, 0.001] # for the statistics\n\nfig = make_figures.make_figure_learning_parameters_between_groups(fit_df, parameters_to_show,\n                                                                  titles, ylabs,\n                                                                  pvals, sig_levels,\n                                                                  color_palette, hue_order, yvals)\n\n# plt.savefig(data_directory + 'Parameters_group_comparison.pdf', transparent=True, bbox_inches='tight')\nplt.show(fig)\n\nfor i,s in enumerate(sig_levels):\n    print(i+1, 'asterisks: pval <', s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Learning curves\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"### calculate the statistical differences of performances between groups at different points in learning\n##### Do not consider trials in which the animal is too biased to calculate performance, as the antibias acts making the mice be worse than chance and that can obscure effects of learning\n\"\"\"\n\n# define the bias threshold as over 2 std of the total dataset\nbias_top_threshold = np.nanmean(df_to_plot.RightBias) + 2 * np.nanstd(df_to_plot.RightBias)\nbias_bottom_threshold = np.nanmean(df_to_plot.RightBias) - 2 * np.nanstd(df_to_plot.RightBias)\n# create a mask\nbias_mask = np.logical_and(df_to_plot.RightBias < bias_top_threshold, df_to_plot.RightBias > bias_bottom_threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sanity check on the data to see that it looks good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = make_figures.make_figure_performance_trials_animals_biased_trials(df_to_plot, bias_mask)\nclear_output()\nplt.show(fig)\n# plt.savefig(data_directory + 'Sigmoid_fitting.pdf', transparent=True, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"#### Recalculate performance\"\"\"\n\nPAST_WINDOW = 100\nCumPerList = []\nfor Sid in pd.unique(df_to_plot['SessionID']):\n    CumPerList.append(cuf.perf_window_calculator(df_to_plot[np.logical_and(bias_mask,\n                                                                           df_to_plot['SessionID']==Sid)],\n                                                 PAST_WINDOW))\n# flatten the list of lists\ndf_to_plot.loc[bias_mask, 'CurrentPastPerformance100biasremoved'] = np.array(list(chain(*[x for x in CumPerList])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"##### The following cell creates the first part of **Figure 1 I** from the paper\"\"\"\n\n# Performance differences between groups across learning\ncol_to_plot = 'CurrentPastPerformance100biasremoved'\nfig1 = make_figures.make_figure_differences_performance_between_groups(df_to_plot, col_to_plot,\n                                                                       hue_order, color_palette)\n\n# plt.savefig(data_directory + 'Performance_between_groups.pdf', transparent=True, bbox_inches='tight')\nfig1.show()\nprint('Shaded area indicates std, and performance is calculated using', col_to_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Separate performance on high and low tones\ncreate a mask for the TrialHighPerc columns\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "high_mask = df_to_plot['TrialHighPerc'] == 98.0\nlow_mask = df_to_plot['TrialHighPerc'] == 2.0\n\n# calculate the performance taking on account only these trials\nCumPerListHigh = []\nCumPerListLow = []\nfor Sid in pd.unique(df_to_plot['SessionID']):\n    CumPerListHigh.append(cuf.perf_window_calculator(df_to_plot[np.logical_and(high_mask,\n                                                                               df_to_plot['SessionID']==Sid)],\n                                                     PAST_WINDOW))\n    CumPerListLow.append(cuf.perf_window_calculator(df_to_plot[np.logical_and(low_mask,\n                                                                              df_to_plot['SessionID']==Sid)],\n                                                    PAST_WINDOW))\n# flatten the list of lists\ndf_to_plot.loc[high_mask, 'CurrentPastPerformance100biasremovedHigh'] = np.array(list(chain(*[x for x in CumPerListHigh])))\ndf_to_plot.loc[low_mask, 'CurrentPastPerformance100biasremovedLow'] = np.array(list(chain(*[x for x in CumPerListLow])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_to_plot[['CurrentPastPerformance100biasremovedHigh', 'CurrentPastPerformance100biasremovedLow']].tail(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot performance on high and low tones\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "col_to_plot = 'CurrentPastPerformance100biasremovedLow'\nfigX = make_figures.make_figure_differences_performance_between_groups(df_to_plot, col_to_plot,\n                                                                       hue_order, color_palette)\n\n# plt.savefig(data_directory + 'Performance_between_groups.pdf', transparent=True, bbox_inches='tight')\nfigX.show()\n\n\"\"\"\nThis approach doesn't seem to work well. It might be because I am using both trials to calculate performance.\nLet's eliminate those trials from the dataset\n\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_high = df_to_plot[high_mask].copy()\ndf_low = df_to_plot[low_mask].copy()\n\n# reset the index and the cumulative trial number by protocol\ndf_high.reset_index(inplace=True)\ndf_low.reset_index(inplace=True)\ndf_high.drop('index', axis=1, inplace=True)\ndf_low.drop('index', axis=1, inplace=True)\n\n# Restart the count of CumulativeTrialNumber for every protocol\ndf_high['CumulativeTrialNumberByProtocol'] = np.nan\ndf_low['CumulativeTrialNumberByProtocol'] = np.nan\n\nfor Aid in pd.unique(df_high['AnimalID']):\n    for Prot in pd.unique(df_high['Protocol']):\n        conditions = np.logical_and(df_high['AnimalID'] == Aid, df_high['Protocol'] == Prot)\n        df_high.CumulativeTrialNumberByProtocol.loc[df_high[conditions].index] = \\\n            np.arange(len(df_high[conditions])) + 1\n\nfor Aid in pd.unique(df_low['AnimalID']):\n    for Prot in pd.unique(df_low['Protocol']):\n        conditions = np.logical_and(df_low['AnimalID'] == Aid, df_low['Protocol'] == Prot)\n        df_low.CumulativeTrialNumberByProtocol.loc[df_low[conditions].index] = \\\n            np.arange(len(df_low[conditions])) + 1\n\n\n# calculate performance\nPAST_WINDOW = 50\nCumPerListHigh = []\nCumPerListLow = []\nfor Sid in pd.unique(df_to_plot['SessionID']):\n    CumPerListHigh.append(cuf.perf_window_calculator(df_high[df_high['SessionID']==Sid],\n                                                     PAST_WINDOW))\n    CumPerListLow.append(cuf.perf_window_calculator(df_low[df_low['SessionID']==Sid],\n                                                     PAST_WINDOW))\n# flatten the list of lists\ndf_high['CurrentPastPerformance100biasremoved'] = np.array(list(chain(*[x for x in CumPerListHigh])))\ndf_low['CurrentPastPerformance100biasremoved'] = np.array(list(chain(*[x for x in CumPerListLow])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot performance on high and low tones\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "col_to_plot = 'CurrentPastPerformance100biasremoved'\nfigX = make_figures.make_figure_differences_performance_between_groups(df_low, col_to_plot,\n                                                                       hue_order, color_palette)\ndata_directory = ''\nplt.savefig(data_directory + 'Chronic_lesions_Performance_between_groups_low_tones.pdf', transparent=True, bbox_inches='tight')\nfigX.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"#### Calculate the significance by resampling: suffle the group labels multiple times and calculate the likelihood of observing this data\nThe shuffling respects the proportion of mice in every group.\n\nReferences:\n\nSupplementary figure 4 in here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2562676/\n\nSee also methods here: https://www.biorxiv.org/content/10.1101/716274v3.full\n\"\"\"\n\n# define a 100-trial window to bin the data\nxbin = 100\ndf_to_plot[\"TrialIndexBinned\"] = (df_to_plot.CumulativeTrialNumberByProtocol // xbin) * xbin + xbin / 2\nprint('Trials are binned in groups of', xbin)\n\n# groupby so each animal has a mean of the performance in each bin\ndf_bintr = df_to_plot.groupby(['AnimalID','ExperimentalGroup','TrialIndexBinned','Protocol']).mean().reset_index()\n\n# create a scaled version of the performance\ndf_bintr['Performance'] = df_bintr.FirstPokeCorrect * 100\n\n# calculate the differences of the real means using the binned data\nreal_data_pd = df_bintr[df_bintr.ExperimentalGroup == hue_order[1]].groupby('TrialIndexBinned').mean()['Performance'] -\\\n               df_bintr[df_bintr.ExperimentalGroup == hue_order[0]].groupby('TrialIndexBinned').mean()['Performance']\n\n# Select the amount of times to shuffle. Originally this is done 10,000 times\n# use a smaller number to speed things up\nnsh = 10000\nprint('Data is shuffled', nsh, 'times')\n\n# select the important columns to get the shuffled data\ndf_colsel = df_bintr[['AnimalID', 'ExperimentalGroup', 'TrialIndexBinned', 'Performance']].copy()\n# get the shuffled data\nshrdf = cuf.get_shuffled_means_difference_df(df_colsel, hue_order, nsh)\n\n# calculate the confidence intervals for the shuffled data\npos_ci = shrdf.groupby('TrialIndexBinned').quantile(.95)\nneg_ci = shrdf.groupby('TrialIndexBinned').quantile(.05)\n\n\"\"\"##### The following cell creates the second part of **Figure 1 I** from the paper\n\nThis shows how likely is that each time point crosses 'random' line (point-base significance).\n\"\"\"\n\nfig2 = make_figures.make_figure_differences_performance_significance(real_data_pd, pos_ci, neg_ci)\n\ndata_directory = '../../data/'\nplt.savefig(data_directory + 'Differences_of_means_significance_by_trial_bins.pdf',transparent=True, bbox_inches='tight')\n\nfig2.show()\n\n\"\"\"#### Substitute of the mixed anova: find the likelihood of any point being significant. Shuffle more data and quantify the percentage of times there is a crossing. Generate global bands of confidence\"\"\"\n\nquants_to_test = [0.99, 0.995, 0.996, 0.997, 0.998, 0.999, 0.9999]\nnsh=1000\n\nglobal_sig = cuf.get_shuffled_means_difference_global_significance(df_colsel, shrdf, quants_to_test, nsh, hue_order)\n\n# plot the confidence intervals and print their global p-values:\nfig = make_figures.make_figure_differences_performance_significance_global(real_data_pd, quants_to_test, shrdf, global_sig, nsh)\nfig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}