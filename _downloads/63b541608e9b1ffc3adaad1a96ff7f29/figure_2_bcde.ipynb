{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure_2_BCDE.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/github/HernandoMV/APE_paper/blob/main/docs/figures_notebooks/Figure_2_BCDE.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This notebook performs a statistical analysis on the learning rates between two groups of mice.\n# Performs model fitting of individual mice, comparison of parameter, and statistical analysis\n# between the groups to conclude differences and points of divergence in learning\n\n# Commented out IPython magic to ensure Python compatibility.\n# run this on Colab\n# !rm -rf APE_paper/\n# !git clone https://github.com/HernandoMV/APE_paper.git\n# %pip install mouse-behavior-analysis-tools\n# %cd APE_paper/docs/figures_notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Commented out IPython magic to ensure Python compatibility.\n%load_ext autoreload\n%autoreload 2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom scipy import stats\nfrom IPython.display import clear_output\nfrom itertools import chain\nfrom os.path import exists\nimport urllib.request\n\nfrom mouse_behavior_analysis_tools.utils import custom_functions as cuf\nfrom mouse_behavior_analysis_tools.utils.misc_utils import update_progress\nfrom mouse_behavior_analysis_tools.plot import make_figures\nfrom mouse_behavior_analysis_tools.model import behavior_model as bm\nfrom mouse_behavior_analysis_tools.utils import model_utils\n\nclear_output()\n\n# This dataset has been pre-processed, but conditions have not been selected\n# This preprocessing includes removal of disengaged trials and\n# removal of the first 5 trials of each session\n\n# download data from server\ndataset_name = '6OHDA_dataframe.csv'\nurl = \"https://zenodo.org/record/7261639/files/\" + dataset_name\ndataset_path = '../data/' + dataset_name\n# download if data is not there\nif not exists(dataset_path):\n    print('Downloading data...')\n    urllib.request.urlretrieve(url, dataset_path)\nelse:\n    print('Data already in directory')\n# load\ndf_to_plot = pd.read_csv(dataset_path, index_col=0)\n\n# load the image data\nimage_dataset_name = '6OHDA_image-data.csv'\nurl = \"https://zenodo.org/record/7261639/files/\" + image_dataset_name\nimage_dataset_path = '../data/' + image_dataset_name\n# download if data is not there\nif not exists(image_dataset_path):\n    print('Downloading data...')\n    urllib.request.urlretrieve(url, image_dataset_path)\nelse:\n    print('Data already in directory')\n# load\niar = pd.read_csv(image_dataset_path, index_col=0)\n\n# parameters for the plotting\nhue_order = ['CortexBuffer', '6OHDAtail']\ncolor_palette = [(0.24715576, 0.49918708, 0.57655991),\n             (0.72162039, 0.3669119 , 0.22526315)]\n\nsns.set_palette(color_palette)\n\n# maximum number of trials performed per mouse in the dataset:\ndf_to_plot.groupby(['AnimalID', 'ExperimentalGroup', 'Protocol']).max()['CumulativeTrialNumberByProtocol']\n\n\"\"\"#### Performance by session and 200-sized bins \"\"\"\n\n# bin trials every 200\ndf_to_plot[\"TrialIndexBinned200\"] = (df_to_plot.CumulativeTrialNumberByProtocol // 200) * 200 + 100\n\n# Sanity check on the data to see that it looks good\n\nfig = make_figures.make_figure_performance_trials_animals_bin(df_to_plot)\nclear_output()\nplt.show(fig)\n# uncomment here to save the plot\n# data_directory = ''\n# plt.savefig(data_directory + 'Performance_by_session_individual_animals.pdf',\n#             transparent=True, bbox_inches='tight')\n\n\"\"\"Colored dots show the performance of the past 100 trials using an average running window. Each color represents a distinct session. The black line shows the performance value of the past 200 trials using trial bining.\n\n#### Fit a sigmoid to every mouse to calculate and compare stages and learning rates\n\nThe sigmoid function is (perf_end - 0.5) / (1 + np.exp(-slope * (x - bias))) + 0.5\n\nData is scaled before the fitting and parameters are rescaled afterwards\n\nThe maximum performace possible is defined as the maximum of the median of the trials binned every 200\n\"\"\"\n\n#calculate the maximum performance for every mouse based on the trials binned every 200\ndf_bin200tr = df_to_plot.groupby(['AnimalID','ExperimentalGroup','TrialIndexBinned200','Protocol']).median().reset_index()\nmouse_max_perf = df_bin200tr.groupby('AnimalID').max().reset_index()[['AnimalID', 'CurrentPastPerformance100']]\n\n# fit the model\nfit_df = bm.get_df_of_behavior_model_by_animal(df_to_plot, mouse_max_perf)\n\n# find the steepest point of each slope (max of derivative)\nder_max_dir = bm.get_steepest_point_of_slope(fit_df)\n\n# plot the curves again pointing to the maximum\n# sanity check to see that these scaled values recreate the curves\nfig = make_figures.make_figure_performance_trials_animals_model(df_to_plot, fit_df, der_max_dir)\nclear_output()\nplt.show(fig)\n# plt.savefig(data_directory + 'Sigmoid_fitting.pdf', transparent=True, bbox_inches='tight')\n\n\"\"\"As a learning speed measure I use the point of the maximum learning rate, which is the maximum of the derivative of the fitting, which is also the middle point of the curve (in between 50% and maximum performance)\nThe bias indicates where this point lies in the x axis: at which trial number this point is reached\n\"\"\"\n\n# add the maximum value of the derivative to the fit_df dataframe\nfor key, value in der_max_dir.items():\n    fit_df.loc[fit_df.index[fit_df['AnimalID'] == key].tolist()[0], 'max_of_der'] = value[1]\n\n\"\"\"### Merge with image data\"\"\"\n\nmerged_df = pd.merge(left=fit_df, right=iar, how='left', left_on='AnimalID', right_on='mouse id')[\n    ['AnimalID',\n     'maximum_performance',\n     'bias',\n     'ExperimentalGroup',\n     'max_of_der',\n     'median_performance_reached_in_last_ntrials',\n     'n trials to criterion',\n     'max performance reached',\n     'ratio posterior/anterior']\n]\n\n\"\"\"##### The following cell creates **Figure 2 E** from the paper\"\"\"\n\nfig = make_figures.make_figure_6ohda_lesion_correlation(merged_df, color_palette)\nplt.show(fig)\n# plt.savefig(data_directory + '6ohda_lesion-correlation.pdf', transparent=True, bbox_inches='tight')\n\n# There is one lesioned animal that is very close to the controls in terms of the lesioned area:\nmerged_df[['AnimalID', 'ExperimentalGroup', 'ratio posterior/anterior']]\n\n# Remove this animal from the study\nans_to_remove = ['SomFlp08']\n# ans_to_remove = []\ndf_to_plot = df_to_plot[~df_to_plot.AnimalID.isin(ans_to_remove)].copy()\nfit_df = fit_df[~fit_df.AnimalID.isin(ans_to_remove)].copy()\nprint('Animals removed from the following analysis based on this:', ans_to_remove)\n\n\"\"\"### Differences of parameters between groups\"\"\"\n\n# calculate significance\nparameters_to_show = ['maximum_performance', 'max_of_der']\n# define three levels of significance for the plotting\nsig_levels = [0.05, 0.01, 0.001]\npvals = []\nprint('Kluskal-Wallis tests on the parameters')\nfor var in parameters_to_show:\n    kt = stats.kruskal(fit_df[fit_df.ExperimentalGroup==hue_order[0]][var].dropna(),\n                       fit_df[fit_df.ExperimentalGroup==hue_order[1]][var].dropna())\n    print( var + ':\\t\\tpvalue: ' + str(kt.pvalue) )\n    pvals.append(kt.pvalue)\n\n\"\"\"##### The following cell creates **Figures 2 C, D** from the paper\"\"\"\n\n# compare the parameters between groups\ntitles = ['maximum\\nperformance', 'maximum\\nlearning rate']\nylabs = ['task performance (%)', '\\u0394performance \\u00D7 trials$\\mathregular{^{-1}}$']\nyvals = [70, 0.001] # for the statistics\n\nfig = make_figures.make_figure_learning_parameters_between_groups(fit_df, parameters_to_show,\n                                                                  titles, ylabs,\n                                                                  pvals, sig_levels,\n                                                                  color_palette, hue_order, yvals)\n\n# plt.savefig(data_directory + 'Parameters_group_comparison.pdf', transparent=True, bbox_inches='tight')\nplt.show(fig)\n\nfor i,s in enumerate(sig_levels):\n    print(i+1, 'asterisks: pval <', s)\n\n\n\n\"\"\"### calculate the statistical differences of performances between groups at different points in learning\n\n##### The following cell creates the first part of **Figure 2 B** from the paper\n\"\"\"\n\n# Performance differences between groups across learning\ncol_to_plot = 'CurrentPastPerformance100'\nfig1 = make_figures.make_figure_differences_performance_between_groups(df_to_plot, col_to_plot,\n                                                                       hue_order, color_palette)\n\n# plt.savefig(data_directory + 'Performance_between_groups.pdf', transparent=True, bbox_inches='tight')\nplt.show(fig1)\nprint('Shaded area indicates std, and performance is calculated using', col_to_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Separate performance on high and low tones\ncreate a mask for the TrialHighPerc columns\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "high_mask = df_to_plot['TrialHighPerc'] == 98.0\nlow_mask = df_to_plot['TrialHighPerc'] == 2.0\ndf_high = df_to_plot[high_mask].copy()\ndf_low = df_to_plot[low_mask].copy()\n\n# reset the index and the cumulative trial number by protocol\ndf_high.reset_index(inplace=True)\ndf_low.reset_index(inplace=True)\ndf_high.drop('index', axis=1, inplace=True)\ndf_low.drop('index', axis=1, inplace=True)\n\n# Restart the count of CumulativeTrialNumber for every protocol\ndf_high['CumulativeTrialNumberByProtocol'] = np.nan\ndf_low['CumulativeTrialNumberByProtocol'] = np.nan\n\nfor Aid in pd.unique(df_high['AnimalID']):\n    for Prot in pd.unique(df_high['Protocol']):\n        conditions = np.logical_and(df_high['AnimalID'] == Aid, df_high['Protocol'] == Prot)\n        df_high.CumulativeTrialNumberByProtocol.loc[df_high[conditions].index] = \\\n            np.arange(len(df_high[conditions])) + 1\n\nfor Aid in pd.unique(df_low['AnimalID']):\n    for Prot in pd.unique(df_low['Protocol']):\n        conditions = np.logical_and(df_low['AnimalID'] == Aid, df_low['Protocol'] == Prot)\n        df_low.CumulativeTrialNumberByProtocol.loc[df_low[conditions].index] = \\\n            np.arange(len(df_low[conditions])) + 1\n\n\n# calculate performance\nPAST_WINDOW = 50\nCumPerListHigh = []\nCumPerListLow = []\nfor Sid in pd.unique(df_to_plot['SessionID']):\n    CumPerListHigh.append(cuf.perf_window_calculator(df_high[df_high['SessionID']==Sid],\n                                                     PAST_WINDOW))\n    CumPerListLow.append(cuf.perf_window_calculator(df_low[df_low['SessionID']==Sid],\n                                                     PAST_WINDOW))\n# flatten the list of lists\ndf_high['CurrentPastPerformance100biasremoved'] = np.array(list(chain(*[x for x in CumPerListHigh])))\ndf_low['CurrentPastPerformance100biasremoved'] = np.array(list(chain(*[x for x in CumPerListLow])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot performance on high and low tones\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "col_to_plot = 'CurrentPastPerformance100biasremoved'\nfigX = make_figures.make_figure_differences_performance_between_groups(df_high, col_to_plot,\n                                                                       hue_order, color_palette)\ndata_directory = ''\nplt.savefig(data_directory + '6OHDA_Performance_between_groups_high_tones.pdf', transparent=True, bbox_inches='tight')\nfigX.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"#### Calculate the significance by resampling: suffle the group labels multiple times and calculate the likelihood of observing this data\nThe shuffling respects the proportion of mice in every group.\n\nReferences:\n\nSupplementary figure 4 in here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2562676/\n\nSee also methods here: https://www.biorxiv.org/content/10.1101/716274v3.full\n\"\"\"\n\n# define a 100-trial window to bin the data\nxbin = 100\ndf_to_plot[\"TrialIndexBinned\"] = (df_to_plot.CumulativeTrialNumberByProtocol // xbin) * xbin + xbin / 2\nprint('Trials are binned in groups of', xbin)\n\n# groupby so each animal has a mean of the performance in each bin\ndf_bintr = df_to_plot.groupby(['AnimalID','ExperimentalGroup','TrialIndexBinned','Protocol']).mean().reset_index()\n\n# create a scaled version of the performance\ndf_bintr['Performance'] = df_bintr.FirstPokeCorrect * 100\n\n# calculate the differences of the real means using the binned data\nreal_data_pd = df_bintr[df_bintr.ExperimentalGroup == hue_order[1]].groupby('TrialIndexBinned').mean()['Performance'] -\\\n               df_bintr[df_bintr.ExperimentalGroup == hue_order[0]].groupby('TrialIndexBinned').mean()['Performance']\n\n# Select the amount of times to shuffle. Originally this is done 10,000 times\n# use a smaller number to speed things up\nnsh = 1000\nprint('Data is shuffled', nsh, 'times')\n\n# select the important columns to get the shuffled data\ndf_colsel = df_bintr[['AnimalID', 'ExperimentalGroup', 'TrialIndexBinned', 'Performance']].copy()\n# get the shuffled data\nshrdf = cuf.get_shuffled_means_difference_df(df_colsel, hue_order, nsh)\n\n# calculate the confidence intervals for the shuffled data\npos_ci = shrdf.groupby('TrialIndexBinned').quantile(.95)\nneg_ci = shrdf.groupby('TrialIndexBinned').quantile(.05)\n\n\"\"\"##### The following cell creates the second part of **Figure 2 B** from the paper\n\nThis shows how likely is that each time point crosses 'random' line (point-base significance).\n\"\"\"\n\nfig2 = make_figures.make_figure_differences_performance_significance(real_data_pd, pos_ci, neg_ci)\n# plt.savefig(data_directory + 'Differences_of_means_significance_by_trial_bins.pdf',transparent=True, bbox_inches='tight')\nplt.show(fig2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}